# -*- coding: utf-8 -*-
"""Activation FUNCTIONS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IAI6YowCDvidLUuUyfsdL693M9JuBKdC
"""

# SIGMOID FUNCTION

# Import required libraries :
import numpy as np
import matplotlib.pyplot as plt

# Sigmoid function :
def sigmoid(x):
 return 1/(1+np.exp(-x))

ip = -1
op = sigmoid(ip)
print(op)

# Define a range of x-values
x = np.linspace(-10, 10, 1000) # 1000 points between -10 and 10

# Calculate the corresponding y-values using the sigmoid function
y = sigmoid(x)

# Plot the graph
plt.plot(x, y)

# Add labels and title
plt.xlabel("x")
plt.ylabel("sigmoid(x)")
plt.title("Sigmoid Function Graph")

# Add a grid for readability
plt.grid(True)

# Display the plot
plt.show()

# TANH ACTIVATION

# Tanh function :
def tanh(x):
 return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))

# Calculate the corresponding y-values using the tanh function
y = tanh(x)

# Plot the graph
plt.plot(x, y)

# Add labels and title
plt.xlabel("x")
plt.ylabel("tanh(x)")
plt.title("Tanh Function Graph")

# Add a grid for readability
plt.grid(True)

# Display the plot
plt.show()

# RELU ACTIVATION

# ReLU function :
def relu(x):
  return np.maximum(0, x)

# Calculate the corresponding y-values using the ReLU function
y = relu(x)

# Plot the graph
plt.plot(x, y)

# Add labels and title
plt.xlabel("x")
plt.ylabel("relu(x)")
plt.title("ReLU Function Graph")

# Add a grid for readability
plt.grid(True)

# Display the plot
plt.show()

# SOFTMAX

# softmax function
def softmax(x):
  a = np.exp(x)
  a = a/np.sum(a)
  return a

# Calculate the corresponding y-values using the ReLU function
y = softmax(x)

# Plot the graph
plt.plot(x, y)

# Add labels and title
plt.xlabel("x")
plt.ylabel("soft(x)")
plt.title("Softmax Function Graph")

# Add a grid for readability
plt.grid(True)

# Display the plot
plt.show()

# LINEAR ACTIVATION

def linear(x):
  return x

# Calculate the corresponding y-values using the ReLU function
y = linear(x)

# Plot the graph
plt.plot(x, y)

# Add labels and title
plt.xlabel("x")
plt.ylabel("linear(x)")
plt.title("Linear Function Graph")

# Add a grid for readability
plt.grid(True)

# Display the plot
plt.show()

# STEP ACTIVATION

# Sample input data
x = np.linspace(-5, 5, 100)

# Step activation function (Binary Step)
def step_function(x):
    return np.where(x >= 0, 1, 0)

# Calculate outputs
y = step_function(x)

# Plot the graph
plt.plot(x, y)

# Add labels and title
plt.xlabel("x")
plt.ylabel("step(x)")
plt.title("Step Function Graph")

# Add a grid for readability
plt.grid(True)

# Display the plot
plt.show()

# Signum activation function

# Sample input data
x = np.linspace(-5, 5, 100)

def signum_function(x):
    return np.where(x > 0, 1, np.where(x < 0, -1, 0))

y = signum_function(x)

# Plot the graph
plt.plot(x, y)

# Add labels and title
plt.xlabel("x")
plt.ylabel("Signum(x)")
plt.title("signum Function Graph")

# Add a grid for readability
plt.grid(True)

# Display the plot
plt.show()